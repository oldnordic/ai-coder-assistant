["Transcription successful (Language: en):", "A couple of months ago, I revealed to you my full process for using AI coding assistance to build anything fast. But since then, there have been a few new MCP servers that have come out. And so right now, my workflow for coding anything with AI revolves around three core MCP servers. So I want to cover those with you right now. But then on top of that, I also want to do a live build with you, showing you how we can use these MCP servers together effectively to build anything. You can transform any AI coding workflow with these servers no matter what you want to build. So let's go ahead and dive into that right now. So here is the document, aligning my full process for using AI coding assistance to build anything. I'll link to this in the description of this video. I shared this originally in the other video that I mentioned. Everything in there is the same, except I've updated section four, where I dive into using MCP servers. And so these are the three core servers that I'll cover", "four, where I dive into using MCP servers. And so these are the three core servers that I'll cover in this video. There are some other ones that are worth looking into as well. These are the ones that I use no matter what I am building. And the important thing to note here is that for each of these servers, there are going to be multiple options. So I'll talk about a couple of these alternatives. But obviously I want to focus on just one for each of these three categories. So the first kind of MCP server that you want no matter what is one to bring external knowledge into your AI coding assistant for things like library and tool documentation. So you have SuperBase, MCP, Pidentic AI, all these things that you're building with a lot. These AI coding assistants don't know them very well. And even though we have built-in documentation support for some AI IDEs like Windsurf and Kersher, it doesn't always work the best. And so we want an MCP server that can essentially be the Ragnolage", "it doesn't always work the best. And so we want an MCP server that can essentially be the Ragnolage Base for our AI coder. And the one that I would highly recommend using is one that I actually built myself. It's completely free and open source. I built it for you specifically for bringing it into your AI coding assistant. So you can crawl any website, any documentation, and then use it then as a RagnMCP server for your AI coding assistant. And all of the knowledge that you build as a part of the server is stored in your own private SuperBase. So it's your own knowledge base that you set up and manage for it yourself. Another good option that I've covered on my channel previously is Context 7. Now this isn't going to be building your own private knowledge base like this server is. But this is another good option. It's more out of the box. They have thousands of libraries that are already ingested in their knowledge base that you can leverage immediately again as an MCP server. So that", "ingested in their knowledge base that you can leverage immediately again as an MCP server. So that is the core one that you want. I'm going to be using this one that I built in this video here with Crawl for AI. I'll show you how we can use it to crawl like SuperBase and PIDantic AI. Bring that into our knowledge base. And then the next kind of server is you want one to help you manage your database so that when the AI coding assistant starts a project for you, not only can it write the code, but it can also create everything in the database like new functions, new tables. And SuperBase is the primary one that I use. And so I have this link as well in the description of the video to the SuperBase MCP server. This can create tables for you, list your projects that you have in your SuperBase organization. It can write queries like pretty much any SQL query you could want. They can do that as a tool. So it's just a super powerful way to manage your database with natural language. So as a", "as a tool. So it's just a super powerful way to manage your database with natural language. So as a part of your process for building your application, it can build all the tables as well. Another one that I mentioned before on my channel is Neon. That's a serverless postgres platform that has a similar MCP server. And so depending on the database that you're using, hopefully there will be an MCP server that'll help you manage that with natural language. And I definitely think we're going in a direction where pretty much every single database you could use is going to have an MCP server. So you might not be there yet, but you probably will in the near future. And then the last server that I use, no matter what is one to search the web. And typically I use the Brave MCP server. So I'll have a link available for this as well. The Brave API is pretty affordable and it is very, very powerful. It's the kind of web search that's better than just something that you would have with chat GBT", "It's the kind of web search that's better than just something that you would have with chat GBT or cloud by itself. It has a very AI-centric web search, summarizing things for AI, the way that it pulls information is just perfect for LLMs. Usually I'll use this kind of in tandem with my RIG MCP server. So I'll look in my knowledge base, but then also search the web for supplemental resources. Like sometimes it'll be documentation that wasn't included in the knowledge base, or maybe there's community forum posts that provide example similar to what I want to build. And then the Brave MCP server will help me find that. And if you want to bring these three MCP servers into your own AI coding workflows, which I would highly recommend doing, then the links that I have in the description to each of these also have very detailed instructions. Like for the crawl for AI and RIG MCP server that I built, and the readme, I made sure to walk you through everything as far as installing, setting up", "built, and the readme, I made sure to walk you through everything as far as installing, setting up the project, beginning your database, configured, and ready to crawl websites and put in your knowledge base. You can take care of all that just reading through this step by step. And then for Brave, you just have to create your API key and they have instructions for how you'll set this up in Cloud Desktop or Windsor for cursor, whatever that might be. And then for Superbase, it's the same kind of deal. And the configuration is generally going to look very, very similar between these different MCP servers. And so going into my own Windsor here, I have the configuration set up. And the way that I get to that, by the way, I'll just show you really quickly. So when I'm in the Cascade mode on the right-hand side, you just have to click on the Hammer icon for MCP servers and then click on Configure. And that'll open up this JSON file for the MCP configuration. And there's going to be", "on Configure. And that'll open up this JSON file for the MCP configuration. And there's going to be something very similar for cursor or root code or client, no matter what AI IDE you are using. You'll have something like this where you can add each of the servers individually. And all these servers are completely free, by the way. So this is my crawl for AI one. Once you follow the instructions and you have the server running on SSI in a terminal, you can then cook into it just like this. And then for the Brave search, we're just pulling this through MPX. The only thing we need to set up beforehand here is our Brave API key. And there is a very generous free tier with Brave. And so that's why you can get started for free with Brave, even though there are paid plans. I'm still not paying anything for what I'm using it right here. And then for Superbase, they also have a very generous free tier that you can spin up an instance, have that running indefinitely. So you can use this also", "tier that you can spin up an instance, have that running indefinitely. So you can use this also without paying anything. The only thing you have to set up here is your access token. And in the reme that I link you to, there's instructions for getting this set up as well. So very easy to get all these configured. Once you have that all configured, you just have to save here. Go back into our configuration and click on Refresh. And then that'll load the MCP servers. And you can see all the tools that are exposed from each of these servers and the descriptions for them as well. So we can take a look at everything that our AI coding assistant is now able to do with these tools. And so we're giving superpowers to our AI coders. Now we can dive into using these all together to build something from scratch, leveraging all of these to do something that we definitely couldn't do very easily without them. The sponsor of today's video is DataButton, the AI app builder for businesses. I've been", "them. The sponsor of today's video is DataButton, the AI app builder for businesses. I've been trying it out the past few weeks. I've been seriously impressed just how much of the development lifecycle DataButton takes care of for you. Like they say in their home page, your hunt for a CTO ends here because it takes care of everything from just starting your project all the way to deploying it. When you first begin a project, you can start by describing what you want to build, giving specific requirements. You can even give inspiration as well, screenshots from other web pages. And then you can hook in your integrations, like for authentication, your database, what you want for payments, and then storage as well. And then you can get started. And the DataButton agent will go ahead and start creating a full plan. With all the tasks listed out that it's going to knock down one by one, we have our previews. We can see what it's currently built so far. And then we can interact with our", "have our previews. We can see what it's currently built so far. And then we can interact with our agent on the right-hand side where we provide any feedback or additional tasks we wanted to do. We can kick off each individual task. And it'll ask for certain things like our API keys as it sets up, super base, and stripe, getting all those integrations ready for us. And this takes care of the front end and the back end. So you can deploy agents and APIs behind the scenes as well. And then deploy it all together once your project is done. This takes care of everything. That's why they say that you don't need a CTO anymore. DataButton really is a game changer for lean businesses that are looking to leverage AI to compete with companies that are 10 times their size. So I'll have a link in the description to DataButton. It's free to get started. You can check it out and see how you can use it to really build any app that you need for your business. All right, here we go. Now it is time to", "to really build any app that you need for your business. All right, here we go. Now it is time to build. So we're going to be leveraging my full process for using AI coding assistance. I already have some of the things set up based on this process. So I'll show you what that looks like right now. Then I'll show you specifically how I'm going to be prompting the AI coding assistant to leverage these different MCP servers. And then we'll see it in action as well. And so the first thing is I have my WinSurf rules set up, which by the way, the way you get to that is you just go to additional options on the top right, go to manage memories and you can create global or workspace rules. So I created some workspace rules here, telling you how to leverage my planning and task files, how to use the crawl for AI MCP server, all that high level direction that I don't necessarily want to prompt it to do every single time. But I want it to follow this as it's golden rules. And then I also have my", "to do every single time. But I want it to follow this as it's golden rules. And then I also have my planning file, which I can preview this for you really quickly just so you can see what this looks like. So I use Cloud Desktop to help me make this, just giving me an overview of my project and the different components that I want to build out. So that the AI coding assistant can reference this and just know what I really want it to build. So I don't have to be extremely specific within my prompt when I actually kick off the build of this agent that I'm making here. And so what we are building here as our live demo is a Rags AI agent with PIDANTIC AI and SuperBase. And so we're going to have this local folder that we can put files into. It'll automatically ingest that into our SuperBase Knowledge Base. And then we'll have a simple StreamLit interface for us to interact with the agent and ask questions about our Knowledge Base. So it's a very simple example. Still something we care", "and ask questions about our Knowledge Base. So it's a very simple example. Still something we care about though. I mean, Rags AI agents are just so important. So that's why we're going to build one right now. And then I also have my task file with all of the individual tasks that we want our agent to look here and knock these off one by one and then come back and update when these tasks are complete. And so again, everything that I've already covered in my previous video on my full process, I have that all set up so that now we can just dive right into prompting our AI coding assistant to get started with this build for us. And so I'll show you I have the prompt prepared already. So I'll start at the top, explain everything really quickly, then we'll see it in action. So I'm just starting by saying that I want to build a simple Rang AI agent with patented AI and SuperBase, telling it to read the planning and task files at first as well. And I do specify in the global rules to do this,", "read the planning and task files at first as well. And I do specify in the global rules to do this, but I'm just really making sure that it handles this right now. And then I just describe the different components that I want for my AI agent. And I do go into more detail in the planning and task files for this as well. So just giving the more higher level instructions right here. And then one really powerful thing for using AI coding assistance in general is to give examples whenever you can. And so for the streamlit user interface specifically to interact with our agent, I've seen in the past that AI coding assistance don't always handle this the best. And so something that I'm doing here is I'm giving it an example of a streamlit interface that I've made in the past specifically to work with pedantic AI agents. And so I'm referencing it like this in Windsurf so that it can analyze this file and see exactly how it's going to build this interface for me for this agent. So that's just", "file and see exactly how it's going to build this interface for me for this agent. So that's just another gem that I wanted to drop really quickly for you there using examples is so, so powerful. And then I do yet another example here. So I have the sequel already prepared for a different project where I created a RAC AI agent. So it can use this as a reference as well. While it is using the SuperBase MCP server to create everything for me and SuperBase for this agent specifically. And that's why I tell it here. I tell it to use the SuperBase MCP server to create the necessary database tables, make sure I have the extension enabled for RAC and then using this as an example. And then I continue with the other two servers. So using crawl for AI to get the documentation for pedantic AI and SuperBase, I already have all of that crawled. And so I just had a separate request where I'm like, hey, here is the page for pedantic AI, crawl all those docs and then here's the same thing for", "hey, here is the page for pedantic AI, crawl all those docs and then here's the same thing for SuperBase. So I did that already. And now it has that documentation available. So just perform the RAC queries. And then also use the Brave MCP server. Like I mentioned earlier, just as kind of like a supplemental resource for our knowledge-based lookups. Because sometimes when you search the internet, you'll find things like forum posts that have examples for something like what you're trying to build. Things that can go in tandem with what we've retrieved from our SuperBase knowledge base. So using these two servers together. And then I'm also very explicit here telling it to make sure that it uses these servers at the very start of the process. So it has all that documentation. Before it tries to write any code, we don't want it to look up the documentation after writing all the code because then it never is really leveraging it. And so with that, we can now send in this request and see", "then it never is really leveraging it. And so with that, we can now send in this request and see it in action. And so the first thing that it'll do because we instruct it to do so in the prompt and the global rules is to check our planning and task documents. And so there we go. It's looking at planning. Now it's looking at tasks. And then pretty soon here, because we told it to it early on, it should start using our MCP server. So first it's looking at these examples that we gave, which is good. I wanted to do that as well. Now here we go. Now we're hitting the perform read query endpoint of our MCP tool, getting some of that documentation for Pydantic AI. And it's searching multiple different times with different queries. This is exactly what I wanted to do to make sure it has a very overarching view of everything that it has to know to implement this agent for me. And then also leveraging the Brave MCP server, getting some examples with integrating SuperBase and Pydantic AI", "leveraging the Brave MCP server, getting some examples with integrating SuperBase and Pydantic AI together. And that's a really powerful thing in general, is a lot of times the knowledge base will give you, here's how to use this specific library. But then the Brave search can tell you how to use these libraries together. So yet another reason that we want to use both of these tools in tandem. And so boom, it's done with all the knowledge look up. Now it's going to update the tasks and it's going to start creating the readme. It'll go through the rest of the process here. And this is going to take a few good minutes here to build everything out. And so what I'll do here is pause and come back. Once it has the first version of my agent complete. And boom, there we go. It created the full AI agent for us. It created a readme that describes the entire project structure. Look at how many files it built for us, making really a full AI agent application here with the rag pipeline, the agent", "it built for us, making really a full AI agent application here with the rag pipeline, the agent and everything. We can take a look at these individual files. We have the Pydantic AI agent. We've got our dependencies and we have the definition of the agent itself. This is looking like really, really clean code. We have our prompts. We have our tools for rag. We've got everything for actually processing documents that we upload in a local folder. Bringing that into our super base. We have all the tests as well because I instructed it to write tests in our workspace rules. We have our user interface that we can clearly see that like this is inspired by the example that I gave earlier. I mean, maybe you don't know that as much as me because I've used that example in the past. But yeah, this is looking really, really good. And then one little glitch here isn't made a brand new task file. So it wasn't perfect. And I think there are a couple of bugs that I'll have to fix up here. But still,", "it wasn't perfect. And I think there are a couple of bugs that I'll have to fix up here. But still, it did a really, really good job getting me started. I've got my example environment variable. So it tells me everything that I have to set up with instructions run everything in the readme. Like this feels like a full software in here built this out. Like I'm really, really impressed. And so now what I'm going to do is accept all these changes. I'm going to get everything set up off camera. There's probably a couple of iterations that I'll have to make to make this like really working. And so I'll explain all that after I do it too. But yeah, let me take care of that now. Then I'll show you a full demo with this AI agent that was created for us so fast. All right, so there were a few things that I had to do to make this in tip top shape for us. The first thing is even though I told it in the first prompt to use the SuperBase MCP server to create my tables for me, it didn't actually do", "the first prompt to use the SuperBase MCP server to create my tables for me, it didn't actually do that. And AI coding assistants are kind of unpredictable. Sometimes you have to re-ask them to do certain things for you. So I did that here. You can see that now it used the apply migration tool to create everything with this query that you can see right here. And then going into my SuperBase, I'll actually go to the table right now. I now have this rag pages table. We have ID, URL, chunk number, the embeddings. That's what we actually use for rag. So we're good with the table now. And then there were some issues around how it embedded the documents that I would upload to the interface. So I had to fix that as well. Nothing that was related to SuperBase or PIDantic AI. So it leveraged the documentation effectively. Like it actually had zero errors for anything related to PIDantic AI like in our agent here. And then for SuperBase, like when it actually creates the embeddings and ingest", "in our agent here. And then for SuperBase, like when it actually creates the embeddings and ingest those into our database. So this whole setup script that creates this client with all of our functions to do all of our inserts and lookups. Like all of this was perfect code, just super cool. So I just had to fix some things with mostly the chunker and then just the way that ingest it. So I don't want to get into the details of all that right now. It's not very important. But now I have the interface up and running. Take a look at this. So I have this stream of interface. Let me zoom in on this for you just a little bit here. So you can see it really well. Now I can browse my files and I can upload text or PDF files. Those are the two types that I'm supporting right now. And so I just have this fake company overview generated by Claude that I'm going to use as my mini knowledge base for this simple example. So I'll go ahead and upload this. And it'll tell me that it's processing this", "this simple example. So I'll go ahead and upload this. And it'll tell me that it's processing this file. It's creating all those embeddings and inserting it into SuperBase for me. And boom, there we go. All documents are processed. We've got five chunks for this file. We now have one document in our knowledge base. Now going back to my table, there we go. All right, we have our five chunks here. Take a look at that. This is so cool. And then now I can talk to my agent. Or at first I can even show you, like if I refresh my page, it'll now say, like, yep, you have one document in your knowledge base. And so now I can just send a basic message to the agent, just like, hello, all right, there we go. We even have the text streaming. So it typed it out in like typewriter style. Super cool. Now I can ask you the question where it has to leverage my knowledge base. And I know this is super basic right now, but I just want to show this at a high level. More focusing on my process for using", "right now, but I just want to show this at a high level. More focusing on my process for using these MCP servers. But yeah, just to show you really quick, I'll say, give me an overview of the company. All right, so we'll watch it perform regs. It's going to take a little bit longer now because it's going to search my knowledge base. And boom, there we go. And overview of the company, Neuroverse Studios, founded in 2022. Let's go check that really quick. So yeah, Neuroverse Studios, yep, founded in 2022. All right, so it's leveraging our knowledge base. And it's such a simple example, but it's just so cool that I only had to iterate like 20 minutes off camera. So basically in like less than an hour, I created this full application using these three core MCP servers to guide the entire process for knowledge, web search, and also setting up my database for me. I had to do nothing in super base. And this was all created for me, including the function under the hood that's used for reg. So", "And this was all created for me, including the function under the hood that's used for reg. So very, very cool stuff. So I'll have a link in the description to this full example if you want to play around with it yourself. The main focus though was how we could start this project. Using our three core MCP servers to get our feet on the ground or running. I'd also highly recommend checking out my previous video on my full AI coding assistant process. And also, if you're interested in taking your AI skills to the next level, definitely check out dynamous.ai. It's my early AI adopter community that I started recently for people like you to level up your AI game to transform your career or business. And so with that, if you appreciate this content, and you're looking forward to more things AI coding and AI agents, I would really appreciate a like and a subscribe. And with that, I will see you in the next video.", "Lang me is a tool from the lang chain ecosystem that enables AI agents to develop and maintain long-term memory across conversations. This is Fad Biza and I welcome you to the channel. In this video, we are going to install this lang on our local system and we are going to integrate it with local free Olama based models. Langm allows you to learn from interactions, store important information and continuously improve the responses of LLM over time. It provides memory management capabilities that work with various storage systems and integrates natively with Langraph that gives agents the ability to remember user preferences, past conversations and contextual information that persists beyond individual chat sessions. So let's get it installed. I already have Olama based model present. If you don't know what Ola is, Ola is one of the easiest tool to get started with AI models. It primarily uses a quantized versions of the model, but you can also go with the full precision models. If you", "uses a quantized versions of the model, but you can also go with the full precision models. If you don't know what Olama is, just go to my channel and search with Ola and you should be able to find heaps of videos around it. Installation is fairly simple. Just click on this download button and then from there for Linux run this command. for Windows and Mac just download this executable and run it and that should be it. Now for the models as this is a toolbased model so you would need to go to model tab click on tools and then select any one of these models which has a tool or function call support. what primarily um entails to that whenever user runs a natural language query or natural text prompt it gets converted into a function call or tool call and this is what this model support. So I already have this quen 3 model running on my local system. This is my local system where I'm running Ubuntu and I have one GPU card Nvidia RTX A6000 with 48GB of VRAM courtesy mass compute. If", "running Ubuntu and I have one GPU card Nvidia RTX A6000 with 48GB of VRAM courtesy mass compute. If you're also looking to rent a GPU or VM on very affordable prices, you can find the link to their website in video's description with a discount coupon code of 50% for range of GPUs. Let me clear the screen. Let me quickly show you my Olama model. So you can see that I already have this Quen 3 is 32 billion parameter model. And by the way, one of the best models in recent weeks. So if you're interested, you can also search this model on my uh channel and I have done a full uh video from various angles on this model. Anyway, let me quickly create a virtual environment with Kod. While that gets installed, let me also introduce you to the sponsors of the video who are Camel AI. Kamel is an open-source community focused on building multi-agent infrastructures for finding the scaling laws with applications in data generation, task automation and world simulation. Next, we need to install the", "applications in data generation, task automation and world simulation. Next, we need to install the lang and as I will be using based model. So, I'm installing lang chain-a which provides you an interface to use based model with this lang chain ecosystem. Let's wait for it. It's not that hi-fi. And by the way, if you are interested, you can even go with open model, enthropic models or whatever model uh is supported in lang chain ecosystem, you can easily use it here like vertex AI, bedrock or whatever. Okay, so let's get started. So we have all the prerequisites ready and let me take you to my local system um where I have this code which is showing us as how exactly this thing runs. So what this code is doing it is importing all of these libraries which we have just installed and then it is creating this memory enabled agent using our Olama based model and you can see here this is where it is creating this memory and search tool. Now what exactly this means is that this code is just", "it is creating this memory and search tool. Now what exactly this means is that this code is just creating an agent which is memory enabled and it uses lang me for that. That is all there is to it. So first you can see it is creating an in-memory store that is acting as an agent's memory database configured to use the local nomic embed text model to convert text into searchable vector embedding with 768 dimension. So if you go back here and do list, you will see I also have another model which is nomic embedded uh embed text. This converts our own data into numerical representation which is understood by the model and that is why you would also need this second model. There are various other models too but I prefer nomic. It is slightly better than the others this week I would say because these things keep changing quite rapidly. Now after it has done that it then creates a react agent using create react uh react agent command as you can see here and this is a reasoning agent that can", "create react uh react agent command as you can see here and this is a reasoning agent that can think through problem step by step and decide when to use tools to accomplish task and then we have this create memory um manage memory tool which allows the agent to actively store important information from conversation into memory store and the agent decides what's worth remembering and saves it automatically. ly then we have the search memory tool that lets the agent search through its stored memories to find relevant information when needed during conversation. So let's run it and what will happen is the agent is going to first receive a message about preferring dark mode and the manage memory tool will store this preference. Okay, let's go back. Let me clear the screen and let me quickly run this code here for you. And there you go. It was fairly quick as you can see. So if you remember this uh in this example, the agent first receives a message about preferring dark mode as you can", "this uh in this example, the agent first receives a message about preferring dark mode as you can see in the code here. And then when that's done um the memory uh the manage memory tool stores this preference of the user later when asked about UI preferences the agent uses the search memory tool to find and retrieve the previously stored dark board preference that shows how it maintains context across separate interactions. Next, let me quickly show you a real world example where you can see that this realistic example is showing a personal assistant that builds up knowledge about um oneselves over multiple conversation sessions. So this is the usual memory instantiation which I'm doing creating the assistant. This is a session one where these are few of the session about this user Sarah and then this is a session two next day assistant remembers the details. There are further queries and then another session another session and then it goes on and on. So you can build a full chat", "and then another session another session and then it goes on and on. So you can build a full chat assistant or any sort of application AI powered which will remember the previous context. So if I for example run this thing, let me run this. There you go. You see this is a day one where some memories are being generated and then it is going to store it in the day one and then every memory gets assigned an ID. That is how that is sort of a primary key or unique identifier which gets stored in the database. So let's wait for it. So there you go. As the quen 3 is a thinking model. So the model has replied and you can see that more memories are being generated. The previous context is being used here and it is it will go on and on and you can see that now there is another memory ID here and Sarah is telling that her favorite coffee is this Ethiopian single origin. There you go. So another now the day two is starting and the memory has been generated around her favorite coffee. There you", "now the day two is starting and the memory has been generated around her favorite coffee. There you go. So it is talking about uh now what what is the work schedule and this personal assistant is sort of going to help her out with her schedule because now the AI powered assistant is getting to know the user all the habits and everything. It is a bit slow because I'm just using it in the U file based one. But in the real world, of course, you would have store it in some sort of um a database or stuff. And you see now the assistant powered by Quen 3 is giving more grounded and advanced answers. There you go. So you see based on my record. So she asked what are my communication preferences and it is telling uh the model is telling or the assistant is telling about all the stuff and how do I like my coffee? Remember it was Ethiopian. There you go. So it says the user likes Ethiopian single origin and this was the first library. So this is how you can generate it and then play around with", "origin and this was the first library. So this is how you can generate it and then play around with it. Um I think pretty good quite lightweight and if you have been following my channel you know that I have covered lot of other AI uh based memory tools like and there are heaps of them. I'm not going to name them here but things have become a lot easier which is always a welcome sign. So that's it. Let me know what do you think about this lang me. I will drop the link to it in video description. Please like the video and share it. I will be very grateful. And if you haven't already subscribed, please do so."]
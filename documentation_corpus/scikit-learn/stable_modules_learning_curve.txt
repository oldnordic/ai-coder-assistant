3.5. Validation curves: plotting scores to evaluate models — scikit-learn 1.7.0 documentation
Skip to main content
Back to top
Ctrl
+
K
Install
User Guide
API
Examples
Community
More
Getting Started
Release History
Glossary
Development
FAQ
Support
Related Projects
Roadmap
Governance
About us
GitHub
Choose version
Install
User Guide
API
Examples
Community
Getting Started
Release History
Glossary
Development
FAQ
Support
Related Projects
Roadmap
Governance
About us
GitHub
Choose version
Section Navigation
1. Supervised learning
1.1. Linear Models
1.2. Linear and Quadratic Discriminant Analysis
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.5. Stochastic Gradient Descent
1.6. Nearest Neighbors
1.7. Gaussian Processes
1.8. Cross decomposition
1.9. Naive Bayes
1.10. Decision Trees
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.12. Multiclass and multioutput algorithms
1.13. Feature selection
1.14. Semi-supervised learning
1.15. Isotonic regression
1.16. Probability calibration
1.17. Neural network models (supervised)
2. Unsupervised learning
2.1. Gaussian mixture models
2.2. Manifold learning
2.3. Clustering
2.4. Biclustering
2.5. Decomposing signals in components (matrix factorization problems)
2.6. Covariance estimation
2.7. Novelty and Outlier Detection
2.8. Density Estimation
2.9. Neural network models (unsupervised)
3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.2. Tuning the hyper-parameters of an estimator
3.3. Tuning the decision threshold for class prediction
3.4. Metrics and scoring: quantifying the quality of predictions
3.5. Validation curves: plotting scores to evaluate models
4. Metadata Routing
5. Inspection
5.1. Partial Dependence and Individual Conditional Expectation plots
5.2. Permutation feature importance
6. Visualizations
7. Dataset transformations
7.1. Pipelines and composite estimators
7.2. Feature extraction
7.3. Preprocessing data
7.4. Imputation of missing values
7.5. Unsupervised dimensionality reduction
7.6. Random Projection
7.7. Kernel Approximation
7.8. Pairwise metrics, Affinities and Kernels
7.9. Transforming the prediction target (
y
)
8. Dataset loading utilities
8.1. Toy datasets
8.2. Real world datasets
8.3. Generated datasets
8.4. Loading other datasets
9. Computing with scikit-learn
9.1. Strategies to scale computationally: bigger data
9.2. Computational Performance
9.3. Parallelism, resource management, and configuration
10. Model persistence
11. Common pitfalls and recommended practices
12. Dispatching
12.1. Array API support (experimental)
13. Choosing the right estimator
14. External Resources, Videos and Talks
User Guide
3.
Model selection and evaluation
3.5.
Validation curves: plotting scores to evaluate models
3.5.
Validation curves: plotting scores to evaluate models
#
Every estimator has its advantages and drawbacks. Its generalization error
can be decomposed in terms of bias, variance and noise. The
bias
of an
estimator is its average error for different training sets. The
variance
of an estimator indicates how sensitive it is to varying training sets. Noise
is a property of the data.
In the following plot, we see a function
\(f(x) = \cos (\frac{3}{2} \pi x)\)
and some noisy samples from that function. We use three different estimators
to fit the function: linear regression with polynomial features of degree 1,
4 and 15. We see that the first estimator can at best provide only a poor fit
to the samples and the true function because it is too simple (high bias),
the second estimator approximates it almost perfectly and the last estimator
approximates the training data perfectly but does not fit the true function
very well, i.e. it is very sensitive to varying training data (high variance).
Bias and variance are inherent properties of estimators and we usually have to
select learning algorithms and hyperparameters so that both bias and variance
are as low as possible (see
Bias-variance dilemma
). Another way to reduce
the variance of a model is to use more training data. However, you should only
collect more training data if the true function is too complex to be
approximated by an estimator with a lower variance.
In the simple one-dimensional problem that we have seen in the example it is
easy to see whether the estimator suffers from bias or variance. However, in
high-dimensional spaces, models can become very difficult to visualize. For
this reason, it is often helpful to use the tools described below.
Examples
Underfitting vs. Overfitting
Effect of model regularization on training and test error
Plotting Learning Curves and Checking Models’ Scalability
3.5.1.
Validation curve
#
To validate a model we need a scoring function (see
Metrics and scoring: quantifying the quality of predictions
),
for example accuracy for classifiers. The proper way of choosing multiple
hyperparameters of an estimator is of course grid search or similar methods
(see
Tuning the hyper-parameters of an estimator
) that select the hyperparameter with the maximum score
on a validation set or multiple validation sets. Note that if we optimize
the hyperparameters based on a validation score the validation score is biased
and not a good estimate of the generalization any longer. To get a proper
estimate of the generalization we have to compute the score on another test
set.
However, it is sometimes helpful to plot the influence of a single
hyperparameter on the training score and the validation score to find out
whether the estimator is overfitting or underfitting for some hyperparameter
values.
The function
validation_curve
can help in this case:
>>>
import
numpy
as
np
>>>
from
sklearn.model_selection
import
validation_curve
>>>
from
sklearn.datasets
import
load_iris
>>>
from
sklearn.svm
import
SVC
>>>
np
.
random
.
seed
(
0
)
>>>
X
,
y
=
load_iris
(
return_X_y
=
True
)
>>>
indices
=
np
.
arange
(
y
.
shape
[
0
])
>>>
np
.
random
.
shuffle
(
indices
)
>>>
X
,
y
=
X
[
indices
],
y
[
indices
]
>>>
train_scores
,
valid_scores
=
validation_curve
(
...
SVC
(
kernel
=
"linear"
),
X
,
y
,
param_name
=
"C"
,
param_range
=
np
.
logspace
(
-
7
,
3
,
3
),
...
)
>>>
train_scores
array([[0.90, 0.94, 0.91, 0.89, 0.92],
[0.9 , 0.92, 0.93, 0.92, 0.93],
[0.97, 1   , 0.98, 0.97, 0.99]])
>>>
valid_scores
array([[0.9, 0.9 , 0.9 , 0.96, 0.9 ],
[0.9, 0.83, 0.96, 0.96, 0.93],
[1. , 0.93, 1   , 1   , 0.9 ]])
If you intend to plot the validation curves only, the class
ValidationCurveDisplay
is more direct than
using matplotlib manually on the results of a call to
validation_curve
.
You can use the method
from_estimator
similarly
to
validation_curve
to generate and plot the validation curve:
from
sklearn.datasets
import
load_iris
from
sklearn.model_selection
import
ValidationCurveDisplay
from
sklearn.svm
import
SVC
from
sklearn.utils
import
shuffle
X
,
y
=
load_iris
(
return_X_y
=
True
)
X
,
y
=
shuffle
(
X
,
y
,
random_state
=
0
)
ValidationCurveDisplay
.
from_estimator
(
SVC
(
kernel
=
"linear"
),
X
,
y
,
param_name
=
"C"
,
param_range
=
np
.
logspace
(
-
7
,
3
,
10
)
)
If the training score and the validation score are both low, the estimator will
be underfitting. If the training score is high and the validation score is low,
the estimator is overfitting and otherwise it is working very well. A low
training score and a high validation score is usually not possible.
3.5.2.
Learning curve
#
A learning curve shows the validation and training score of an estimator
for varying numbers of training samples. It is a tool to find out how much
we benefit from adding more training data and whether the estimator suffers
more from a variance error or a bias error. Consider the following example
where we plot the learning curve of a naive Bayes classifier and an SVM.
For the naive Bayes, both the validation score and the training score
converge to a value that is quite low with increasing size of the training
set. Thus, we will probably not benefit much from more training data.
In contrast, for small amounts of data, the training score of the SVM is
much greater than the validation score. Adding more training samples will
most likely increase generalization.
We can use the function
learning_curve
to generate the values
that are required to plot such a learning curve (number of samples
that have been used, the average scores on the training sets and the
average scores on the validation sets):
>>>
from
sklearn.model_selection
import
learning_curve
>>>
from
sklearn.svm
import
SVC
>>>
train_sizes
,
train_scores
,
valid_scores
=
learning_curve
(
...
SVC
(
kernel
=
'linear'
),
X
,
y
,
train_sizes
=
[
50
,
80
,
110
],
cv
=
5
)
>>>
train_sizes
array([ 50, 80, 110])
>>>
train_scores
array([[0.98, 0.98 , 0.98, 0.98, 0.98],
[0.98, 1.   , 0.98, 0.98, 0.98],
[0.98, 1.   , 0.98, 0.98, 0.99]])
>>>
valid_scores
array([[1. ,  0.93,  1. ,  1. ,  0.96],
[1. ,  0.96,  1. ,  1. ,  0.96],
[1. ,  0.96,  1. ,  1. ,  0.96]])
If you intend to plot the learning curves only, the class
LearningCurveDisplay
will be easier to use.
You can use the method
from_estimator
similarly
to
learning_curve
to generate and plot the learning curve:
from
sklearn.datasets
import
load_iris
from
sklearn.model_selection
import
LearningCurveDisplay
from
sklearn.svm
import
SVC
from
sklearn.utils
import
shuffle
X
,
y
=
load_iris
(
return_X_y
=
True
)
X
,
y
=
shuffle
(
X
,
y
,
random_state
=
0
)
LearningCurveDisplay
.
from_estimator
(
SVC
(
kernel
=
"linear"
),
X
,
y
,
train_sizes
=
[
50
,
80
,
110
],
cv
=
5
)
Examples
See
Plotting Learning Curves and Checking Models’ Scalability
for an
example of using learning curves to check the scalability of a predictive model.
previous
3.4.
Metrics and scoring: quantifying the quality of predictions
next
4.
Metadata Routing
On this page
3.5.1. Validation curve
3.5.2. Learning curve
This Page
Show Source
© Copyright 2007 - 2025, scikit-learn developers (BSD License).
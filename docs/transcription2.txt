Lang me is a tool from the lang chain ecosystem that enables AI agents to develop and maintain long-term memory across conversations. This is Fad Biza and I welcome you to the channel. In this video, we are going to install this lang on our local system and we are going to integrate it with local free Olama based models. Langm allows you to learn from interactions, store important information and continuously improve the responses of LLM over time. It provides memory management capabilities that work with various storage systems and integrates natively with Langraph that gives agents the ability to remember user preferences, past conversations and contextual information that persists beyond individual chat sessions. So let's get it installed. I already have Olama based model present. If you don't know what Ola is, Ola is one of the easiest tool to get started with AI models. It primarily uses a quantized versions of the model, but you can also go with the full precision models. If you don't know what Olama is, just go to my channel and search with Ola and you should be able to find heaps of videos around it. Installation is fairly simple. Just click on this download button and then from there for Linux run this command. for Windows and Mac just download this executable and run it and that should be it. Now for the models as this is a toolbased model so you would need to go to model tab click on tools and then select any one of these models which has a tool or function call support. what primarily um entails to that whenever user runs a natural language query or natural text prompt it gets converted into a function call or tool call and this is what this model support. So I already have this quen 3 model running on my local system. This is my local system where I'm running Ubuntu and I have one GPU card Nvidia RTX A6000 with 48GB of VRAM courtesy mass compute. If you're also looking to rent a GPU or VM on very affordable prices, you can find the link to their website in video's description with a discount coupon code of 50% for range of GPUs. Let me clear the screen. Let me quickly show you my Olama model. So you can see that I already have this Quen 3 is 32 billion parameter model. And by the way, one of the best models in recent weeks. So if you're interested, you can also search this model on my uh channel and I have done a full uh video from various angles on this model. Anyway, let me quickly create a virtual environment with Kod. While that gets installed, let me also introduce you to the sponsors of the video who are Camel AI. Kamel is an open-source community focused on building multi-agent infrastructures for finding the scaling laws with applications in data generation, task automation and world simulation. Next, we need to install the lang and as I will be using based model. So, I'm installing lang chain-a which provides you an interface to use based model with this lang chain ecosystem. Let's wait for it. It's not that hi-fi. And by the way, if you are interested, you can even go with open model, enthropic models or whatever model uh is supported in lang chain ecosystem, you can easily use it here like vertex AI, bedrock or whatever. Okay, so let's get started. So we have all the prerequisites ready and let me take you to my local system um where I have this code which is showing us as how exactly this thing runs. So what this code is doing it is importing all of these libraries which we have just installed and then it is creating this memory enabled agent using our Olama based model and you can see here this is where it is creating this memory and search tool. Now what exactly this means is that this code is just creating an agent which is memory enabled and it uses lang me for that. That is all there is to it. So first you can see it is creating an in-memory store that is acting as an agent's memory database configured to use the local nomic embed text model to convert text into searchable vector embedding with 768 dimension. So if you go back here and do list, you will see I also have another model which is nomic embedded uh embed text. This converts our own data into numerical representation which is understood by the model and that is why you would also need this second model. There are various other models too but I prefer nomic. It is slightly better than the others this week I would say because these things keep changing quite rapidly. Now after it has done that it then creates a react agent using create react uh react agent command as you can see here and this is a reasoning agent that can think through problem step by step and decide when to use tools to accomplish task and then we have this create memory um manage memory tool which allows the agent to actively store important information from conversation into memory store and the agent decides what's worth remembering and saves it automatically. ly then we have the search memory tool that lets the agent search through its stored memories to find relevant information when needed during conversation. So let's run it and what will happen is the agent is going to first receive a message about preferring dark mode and the manage memory tool will store this preference. Okay, let's go back. Let me clear the screen and let me quickly run this code here for you. And there you go. It was fairly quick as you can see. So if you remember this uh in this example, the agent first receives a message about preferring dark mode as you can see in the code here. And then when that's done um the memory uh the manage memory tool stores this preference of the user later when asked about UI preferences the agent uses the search memory tool to find and retrieve the previously stored dark board preference that shows how it maintains context across separate interactions. Next, let me quickly show you a real world example where you can see that this realistic example is showing a personal assistant that builds up knowledge about um oneselves over multiple conversation sessions. So this is the usual memory instantiation which I'm doing creating the assistant. This is a session one where these are few of the session about this user Sarah and then this is a session two next day assistant remembers the details. There are further queries and then another session another session and then it goes on and on. So you can build a full chat assistant or any sort of application AI powered which will remember the previous context. So if I for example run this thing, let me run this. There you go. You see this is a day one where some memories are being generated and then it is going to store it in the day one and then every memory gets assigned an ID. That is how that is sort of a primary key or unique identifier which gets stored in the database. So let's wait for it. So there you go. As the quen 3 is a thinking model. So the model has replied and you can see that more memories are being generated. The previous context is being used here and it is it will go on and on and you can see that now there is another memory ID here and Sarah is telling that her favorite coffee is this Ethiopian single origin. There you go. So another now the day two is starting and the memory has been generated around her favorite coffee. There you go. So it is talking about uh now what what is the work schedule and this personal assistant is sort of going to help her out with her schedule because now the AI powered assistant is getting to know the user all the habits and everything. It is a bit slow because I'm just using it in the U file based one. But in the real world, of course, you would have store it in some sort of um a database or stuff. And you see now the assistant powered by Quen 3 is giving more grounded and advanced answers. There you go. So you see based on my record. So she asked what are my communication preferences and it is telling uh the model is telling or the assistant is telling about all the stuff and how do I like my coffee? Remember it was Ethiopian. There you go. So it says the user likes Ethiopian single origin and this was the first library. So this is how you can generate it and then play around with it. Um I think pretty good quite lightweight and if you have been following my channel you know that I have covered lot of other AI uh based memory tools like and there are heaps of them. I'm not going to name them here but things have become a lot easier which is always a welcome sign. So that's it. Let me know what do you think about this lang me. I will drop the link to it in video description. Please like the video and share it. I will be very grateful. And if you haven't already subscribed, please do so.

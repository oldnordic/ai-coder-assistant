User Guide — scikit-learn 1.7.0 documentation
Skip to main content
Back to top
Ctrl
+
K
Install
User Guide
API
Examples
Community
More
Getting Started
Release History
Glossary
Development
FAQ
Support
Related Projects
Roadmap
Governance
About us
GitHub
Choose version
Install
User Guide
API
Examples
Community
Getting Started
Release History
Glossary
Development
FAQ
Support
Related Projects
Roadmap
Governance
About us
GitHub
Choose version
Section Navigation
1. Supervised learning
1.1. Linear Models
1.2. Linear and Quadratic Discriminant Analysis
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.5. Stochastic Gradient Descent
1.6. Nearest Neighbors
1.7. Gaussian Processes
1.8. Cross decomposition
1.9. Naive Bayes
1.10. Decision Trees
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.12. Multiclass and multioutput algorithms
1.13. Feature selection
1.14. Semi-supervised learning
1.15. Isotonic regression
1.16. Probability calibration
1.17. Neural network models (supervised)
2. Unsupervised learning
2.1. Gaussian mixture models
2.2. Manifold learning
2.3. Clustering
2.4. Biclustering
2.5. Decomposing signals in components (matrix factorization problems)
2.6. Covariance estimation
2.7. Novelty and Outlier Detection
2.8. Density Estimation
2.9. Neural network models (unsupervised)
3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.2. Tuning the hyper-parameters of an estimator
3.3. Tuning the decision threshold for class prediction
3.4. Metrics and scoring: quantifying the quality of predictions
3.5. Validation curves: plotting scores to evaluate models
4. Metadata Routing
5. Inspection
5.1. Partial Dependence and Individual Conditional Expectation plots
5.2. Permutation feature importance
6. Visualizations
7. Dataset transformations
7.1. Pipelines and composite estimators
7.2. Feature extraction
7.3. Preprocessing data
7.4. Imputation of missing values
7.5. Unsupervised dimensionality reduction
7.6. Random Projection
7.7. Kernel Approximation
7.8. Pairwise metrics, Affinities and Kernels
7.9. Transforming the prediction target (
y
)
8. Dataset loading utilities
8.1. Toy datasets
8.2. Real world datasets
8.3. Generated datasets
8.4. Loading other datasets
9. Computing with scikit-learn
9.1. Strategies to scale computationally: bigger data
9.2. Computational Performance
9.3. Parallelism, resource management, and configuration
10. Model persistence
11. Common pitfalls and recommended practices
12. Dispatching
12.1. Array API support (experimental)
13. Choosing the right estimator
14. External Resources, Videos and Talks
User Guide
User Guide
#
1. Supervised learning
1.1. Linear Models
1.1.1. Ordinary Least Squares
1.1.2. Ridge regression and classification
1.1.3. Lasso
1.1.4. Multi-task Lasso
1.1.5. Elastic-Net
1.1.6. Multi-task Elastic-Net
1.1.7. Least Angle Regression
1.1.8. LARS Lasso
1.1.9. Orthogonal Matching Pursuit (OMP)
1.1.10. Bayesian Regression
1.1.11. Logistic regression
1.1.12. Generalized Linear Models
1.1.13. Stochastic Gradient Descent - SGD
1.1.14. Perceptron
1.1.15. Passive Aggressive Algorithms
1.1.16. Robustness regression: outliers and modeling errors
1.1.17. Quantile Regression
1.1.18. Polynomial regression: extending linear models with basis functions
1.2. Linear and Quadratic Discriminant Analysis
1.2.1. Dimensionality reduction using Linear Discriminant Analysis
1.2.2. Mathematical formulation of the LDA and QDA classifiers
1.2.3. Mathematical formulation of LDA dimensionality reduction
1.2.4. Shrinkage and Covariance Estimator
1.2.5. Estimation algorithms
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.4.1. Classification
1.4.2. Regression
1.4.3. Density estimation, novelty detection
1.4.4. Complexity
1.4.5. Tips on Practical Use
1.4.6. Kernel functions
1.4.7. Mathematical formulation
1.4.8. Implementation details
1.5. Stochastic Gradient Descent
1.5.1. Classification
1.5.2. Regression
1.5.3. Online One-Class SVM
1.5.4. Stochastic Gradient Descent for sparse data
1.5.5. Complexity
1.5.6. Stopping criterion
1.5.7. Tips on Practical Use
1.5.8. Mathematical formulation
1.5.9. Implementation details
1.6. Nearest Neighbors
1.6.1. Unsupervised Nearest Neighbors
1.6.2. Nearest Neighbors Classification
1.6.3. Nearest Neighbors Regression
1.6.4. Nearest Neighbor Algorithms
1.6.5. Nearest Centroid Classifier
1.6.6. Nearest Neighbors Transformer
1.6.7. Neighborhood Components Analysis
1.7. Gaussian Processes
1.7.1. Gaussian Process Regression (GPR)
1.7.2. Gaussian Process Classification (GPC)
1.7.3. GPC examples
1.7.4. Kernels for Gaussian Processes
1.8. Cross decomposition
1.8.1. PLSCanonical
1.8.2. PLSSVD
1.8.3. PLSRegression
1.8.4. Canonical Correlation Analysis
1.9. Naive Bayes
1.9.1. Gaussian Naive Bayes
1.9.2. Multinomial Naive Bayes
1.9.3. Complement Naive Bayes
1.9.4. Bernoulli Naive Bayes
1.9.5. Categorical Naive Bayes
1.9.6. Out-of-core naive Bayes model fitting
1.10. Decision Trees
1.10.1. Classification
1.10.2. Regression
1.10.3. Multi-output problems
1.10.4. Complexity
1.10.5. Tips on practical use
1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART
1.10.7. Mathematical formulation
1.10.8. Missing Values Support
1.10.9. Minimal Cost-Complexity Pruning
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.11.1. Gradient-boosted trees
1.11.2. Random forests and other randomized tree ensembles
1.11.3. Bagging meta-estimator
1.11.4. Voting Classifier
1.11.5. Voting Regressor
1.11.6. Stacked generalization
1.11.7. AdaBoost
1.12. Multiclass and multioutput algorithms
1.12.1. Multiclass classification
1.12.2. Multilabel classification
1.12.3. Multiclass-multioutput classification
1.12.4. Multioutput regression
1.13. Feature selection
1.13.1. Removing features with low variance
1.13.2. Univariate feature selection
1.13.3. Recursive feature elimination
1.13.4. Feature selection using SelectFromModel
1.13.5. Sequential Feature Selection
1.13.6. Feature selection as part of a pipeline
1.14. Semi-supervised learning
1.14.1. Self Training
1.14.2. Label Propagation
1.15. Isotonic regression
1.16. Probability calibration
1.16.1. Calibration curves
1.16.2. Calibrating a classifier
1.16.3. Usage
1.17. Neural network models (supervised)
1.17.1. Multi-layer Perceptron
1.17.2. Classification
1.17.3. Regression
1.17.4. Regularization
1.17.5. Algorithms
1.17.6. Complexity
1.17.7. Tips on Practical Use
1.17.8. More control with warm_start
2. Unsupervised learning
2.1. Gaussian mixture models
2.1.1. Gaussian Mixture
2.1.2. Variational Bayesian Gaussian Mixture
2.2. Manifold learning
2.2.1. Introduction
2.2.2. Isomap
2.2.3. Locally Linear Embedding
2.2.4. Modified Locally Linear Embedding
2.2.5. Hessian Eigenmapping
2.2.6. Spectral Embedding
2.2.7. Local Tangent Space Alignment
2.2.8. Multi-dimensional Scaling (MDS)
2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)
2.2.10. Tips on practical use
2.3. Clustering
2.3.1. Overview of clustering methods
2.3.2. K-means
2.3.3. Affinity Propagation
2.3.4. Mean Shift
2.3.5. Spectral clustering
2.3.6. Hierarchical clustering
2.3.7. DBSCAN
2.3.8. HDBSCAN
2.3.9. OPTICS
2.3.10. BIRCH
2.3.11. Clustering performance evaluation
2.4. Biclustering
2.4.1. Spectral Co-Clustering
2.4.2. Spectral Biclustering
2.4.3. Biclustering evaluation
2.5. Decomposing signals in components (matrix factorization problems)
2.5.1. Principal component analysis (PCA)
2.5.2. Kernel Principal Component Analysis (kPCA)
2.5.3. Truncated singular value decomposition and latent semantic analysis
2.5.4. Dictionary Learning
2.5.5. Factor Analysis
2.5.6. Independent component analysis (ICA)
2.5.7. Non-negative matrix factorization (NMF or NNMF)
2.5.8. Latent Dirichlet Allocation (LDA)
2.6. Covariance estimation
2.6.1. Empirical covariance
2.6.2. Shrunk Covariance
2.6.3. Sparse inverse covariance
2.6.4. Robust Covariance Estimation
2.7. Novelty and Outlier Detection
2.7.1. Overview of outlier detection methods
2.7.2. Novelty Detection
2.7.3. Outlier Detection
2.7.4. Novelty detection with Local Outlier Factor
2.8. Density Estimation
2.8.1. Density Estimation: Histograms
2.8.2. Kernel Density Estimation
2.9. Neural network models (unsupervised)
2.9.1. Restricted Boltzmann machines
3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.1.1. Computing cross-validated metrics
3.1.2. Cross validation iterators
3.1.3. A note on shuffling
3.1.4. Cross validation and model selection
3.1.5. Permutation test score
3.2. Tuning the hyper-parameters of an estimator
3.2.1. Exhaustive Grid Search
3.2.2. Randomized Parameter Optimization
3.2.3. Searching for optimal parameters with successive halving
3.2.4. Tips for parameter search
3.2.5. Alternatives to brute force parameter search
3.3. Tuning the decision threshold for class prediction
3.3.1. Post-tuning the decision threshold
3.4. Metrics and scoring: quantifying the quality of predictions
3.4.1. Which scoring function should I use?
3.4.2. Scoring API overview
3.4.3. The
scoring
parameter: defining model evaluation rules
3.4.4. Classification metrics
3.4.5. Multilabel ranking metrics
3.4.6. Regression metrics
3.4.7. Clustering metrics
3.4.8. Dummy estimators
3.5. Validation curves: plotting scores to evaluate models
3.5.1. Validation curve
3.5.2. Learning curve
4. Metadata Routing
4.1. Usage Examples
4.1.1. Weighted scoring and fitting
4.1.2. Weighted scoring and unweighted fitting
4.1.3. Unweighted feature selection
4.1.4. Different scoring and fitting weights
4.2. API Interface
4.3. Metadata Routing Support Status
5. Inspection
5.1. Partial Dependence and Individual Conditional Expectation plots
5.1.1. Partial dependence plots
5.1.2. Individual conditional expectation (ICE) plot
5.1.3. Mathematical Definition
5.1.4. Computation methods
5.2. Permutation feature importance
5.2.1. Outline of the permutation importance algorithm
5.2.2. Relation to impurity-based importance in trees
5.2.3. Misleading values on strongly correlated features
6. Visualizations
6.1. Available Plotting Utilities
6.1.1. Display Objects
7. Dataset transformations
7.1. Pipelines and composite estimators
7.1.1. Pipeline: chaining estimators
7.1.2. Transforming target in regression
7.1.3. FeatureUnion: composite feature spaces
7.1.4. ColumnTransformer for heterogeneous data
7.1.5. Visualizing Composite Estimators
7.2. Feature extraction
7.2.1. Loading features from dicts
7.2.2. Feature hashing
7.2.3. Text feature extraction
7.2.4. Image feature extraction
7.3. Preprocessing data
7.3.1. Standardization, or mean removal and variance scaling
7.3.2. Non-linear transformation
7.3.3. Normalization
7.3.4. Encoding categorical features
7.3.5. Discretization
7.3.6. Imputation of missing values
7.3.7. Generating polynomial features
7.3.8. Custom transformers
7.4. Imputation of missing values
7.4.1. Univariate vs. Multivariate Imputation
7.4.2. Univariate feature imputation
7.4.3. Multivariate feature imputation
7.4.4. Nearest neighbors imputation
7.4.5. Keeping the number of features constant
7.4.6. Marking imputed values
7.4.7. Estimators that handle NaN values
7.5. Unsupervised dimensionality reduction
7.5.1. PCA: principal component analysis
7.5.2. Random projections
7.5.3. Feature agglomeration
7.6. Random Projection
7.6.1. The Johnson-Lindenstrauss lemma
7.6.2. Gaussian random projection
7.6.3. Sparse random projection
7.6.4. Inverse Transform
7.7. Kernel Approximation
7.7.1. Nystroem Method for Kernel Approximation
7.7.2. Radial Basis Function Kernel
7.7.3. Additive Chi Squared Kernel
7.7.4. Skewed Chi Squared Kernel
7.7.5. Polynomial Kernel Approximation via Tensor Sketch
7.7.6. Mathematical Details
7.8. Pairwise metrics, Affinities and Kernels
7.8.1. Cosine similarity
7.8.2. Linear kernel
7.8.3. Polynomial kernel
7.8.4. Sigmoid kernel
7.8.5. RBF kernel
7.8.6. Laplacian kernel
7.8.7. Chi-squared kernel
7.9. Transforming the prediction target (
y
)
7.9.1. Label binarization
7.9.2. Label encoding
8. Dataset loading utilities
8.1. Toy datasets
8.1.1. Iris plants dataset
8.1.2. Diabetes dataset
8.1.3. Optical recognition of handwritten digits dataset
8.1.4. Linnerrud dataset
8.1.5. Wine recognition dataset
8.1.6. Breast cancer Wisconsin (diagnostic) dataset
8.2. Real world datasets
8.2.1. The Olivetti faces dataset
8.2.2. The 20 newsgroups text dataset
8.2.3. The Labeled Faces in the Wild face recognition dataset
8.2.4. Forest covertypes
8.2.5. RCV1 dataset
8.2.6. Kddcup 99 dataset
8.2.7. California Housing dataset
8.2.8. Species distribution dataset
8.3. Generated datasets
8.3.1. Generators for classification and clustering
8.3.2. Generators for regression
8.3.3. Generators for manifold learning
8.3.4. Generators for decomposition
8.4. Loading other datasets
8.4.1. Sample images
8.4.2. Datasets in svmlight / libsvm format
8.4.3. Downloading datasets from the openml.org repository
8.4.4. Loading from external datasets
9. Computing with scikit-learn
9.1. Strategies to scale computationally: bigger data
9.1.1. Scaling with instances using out-of-core learning
9.2. Computational Performance
9.2.1. Prediction Latency
9.2.2. Prediction Throughput
9.2.3. Tips and Tricks
9.3. Parallelism, resource management, and configuration
9.3.1. Parallelism
9.3.2. Configuration switches
10. Model persistence
10.1. Workflow Overview
10.1.1. Train and Persist the Model
10.2. ONNX
10.3.
skops.io
10.4.
pickle
,
joblib
, and
cloudpickle
10.5. Security & Maintainability Limitations
10.5.1. Replicating the training environment in production
10.5.2. Serving the model artifact
10.6. Summarizing the key points
11. Common pitfalls and recommended practices
11.1. Inconsistent preprocessing
11.2. Data leakage
11.2.1. How to avoid data leakage
11.2.2. Data leakage during pre-processing
11.3. Controlling randomness
11.3.1. Using
None
or
RandomState
instances, and repeated calls to
fit
and
split
11.3.2. Common pitfalls and subtleties
11.3.3. General recommendations
12. Dispatching
12.1. Array API support (experimental)
12.1.1. Example usage
12.1.2. Support for
Array
API
-compatible inputs
12.1.3. Common estimator checks
13. Choosing the right estimator
14. External Resources, Videos and Talks
14.1. The scikit-learn MOOC
14.2. Videos
14.3. New to Scientific Python?
14.4. External Tutorials
previous
Installing scikit-learn
next
1.
Supervised learning
This Page
Show Source
© Copyright 2007 - 2025, scikit-learn developers (BSD License).
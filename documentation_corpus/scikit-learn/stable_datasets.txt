8. Dataset loading utilities — scikit-learn 1.7.0 documentation
Skip to main content
Back to top
Ctrl
+
K
Install
User Guide
API
Examples
Community
More
Getting Started
Release History
Glossary
Development
FAQ
Support
Related Projects
Roadmap
Governance
About us
GitHub
Choose version
Install
User Guide
API
Examples
Community
Getting Started
Release History
Glossary
Development
FAQ
Support
Related Projects
Roadmap
Governance
About us
GitHub
Choose version
Section Navigation
1. Supervised learning
1.1. Linear Models
1.2. Linear and Quadratic Discriminant Analysis
1.3. Kernel ridge regression
1.4. Support Vector Machines
1.5. Stochastic Gradient Descent
1.6. Nearest Neighbors
1.7. Gaussian Processes
1.8. Cross decomposition
1.9. Naive Bayes
1.10. Decision Trees
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking
1.12. Multiclass and multioutput algorithms
1.13. Feature selection
1.14. Semi-supervised learning
1.15. Isotonic regression
1.16. Probability calibration
1.17. Neural network models (supervised)
2. Unsupervised learning
2.1. Gaussian mixture models
2.2. Manifold learning
2.3. Clustering
2.4. Biclustering
2.5. Decomposing signals in components (matrix factorization problems)
2.6. Covariance estimation
2.7. Novelty and Outlier Detection
2.8. Density Estimation
2.9. Neural network models (unsupervised)
3. Model selection and evaluation
3.1. Cross-validation: evaluating estimator performance
3.2. Tuning the hyper-parameters of an estimator
3.3. Tuning the decision threshold for class prediction
3.4. Metrics and scoring: quantifying the quality of predictions
3.5. Validation curves: plotting scores to evaluate models
4. Metadata Routing
5. Inspection
5.1. Partial Dependence and Individual Conditional Expectation plots
5.2. Permutation feature importance
6. Visualizations
7. Dataset transformations
7.1. Pipelines and composite estimators
7.2. Feature extraction
7.3. Preprocessing data
7.4. Imputation of missing values
7.5. Unsupervised dimensionality reduction
7.6. Random Projection
7.7. Kernel Approximation
7.8. Pairwise metrics, Affinities and Kernels
7.9. Transforming the prediction target (
y
)
8. Dataset loading utilities
8.1. Toy datasets
8.2. Real world datasets
8.3. Generated datasets
8.4. Loading other datasets
9. Computing with scikit-learn
9.1. Strategies to scale computationally: bigger data
9.2. Computational Performance
9.3. Parallelism, resource management, and configuration
10. Model persistence
11. Common pitfalls and recommended practices
12. Dispatching
12.1. Array API support (experimental)
13. Choosing the right estimator
14. External Resources, Videos and Talks
User Guide
8.
Dataset loading utilities
8.
Dataset loading utilities
#
The
sklearn.datasets
package embeds some small toy datasets and provides helpers
to fetch larger datasets commonly used by the machine learning community to benchmark
algorithms on data that comes from the ‘real world’.
To evaluate the impact of the scale of the dataset (
n_samples
and
n_features
) while controlling the statistical properties of the data
(typically the correlation and informativeness of the features), it is
also possible to generate synthetic data.
General dataset API.
There are three main kinds of dataset interfaces that
can be used to get datasets depending on the desired type of dataset.
The dataset loaders.
They can be used to load small standard datasets,
described in the
Toy datasets
section.
The dataset fetchers.
They can be used to download and load larger datasets,
described in the
Real world datasets
section.
Both loaders and fetchers functions return a
Bunch
object holding at least two items:
an array of shape
n_samples
*
n_features
with
key
data
(except for 20newsgroups) and a numpy array of
length
n_samples
, containing the target values, with key
target
.
The Bunch object is a dictionary that exposes its keys as attributes.
For more information about Bunch object, see
Bunch
.
It’s also possible for almost all of these functions to constrain the output
to be a tuple containing only the data and the target, by setting the
return_X_y
parameter to
True
.
The datasets also contain a full description in their
DESCR
attribute and
some contain
feature_names
and
target_names
. See the dataset
descriptions below for details.
The dataset generation functions.
They can be used to generate controlled
synthetic datasets, described in the
Generated datasets
section.
These functions return a tuple
(X,
y)
consisting of a
n_samples
*
n_features
numpy array
X
and an array of length
n_samples
containing the targets
y
.
In addition, there are also miscellaneous tools to load datasets of other
formats or from other locations, described in the
Loading other datasets
section.
8.1. Toy datasets
8.1.1. Iris plants dataset
8.1.2. Diabetes dataset
8.1.3. Optical recognition of handwritten digits dataset
8.1.4. Linnerrud dataset
8.1.5. Wine recognition dataset
8.1.6. Breast cancer Wisconsin (diagnostic) dataset
8.2. Real world datasets
8.2.1. The Olivetti faces dataset
8.2.2. The 20 newsgroups text dataset
8.2.3. The Labeled Faces in the Wild face recognition dataset
8.2.4. Forest covertypes
8.2.5. RCV1 dataset
8.2.6. Kddcup 99 dataset
8.2.7. California Housing dataset
8.2.8. Species distribution dataset
8.3. Generated datasets
8.3.1. Generators for classification and clustering
8.3.2. Generators for regression
8.3.3. Generators for manifold learning
8.3.4. Generators for decomposition
8.4. Loading other datasets
8.4.1. Sample images
8.4.2. Datasets in svmlight / libsvm format
8.4.3. Downloading datasets from the openml.org repository
8.4.4. Loading from external datasets
previous
7.9.
Transforming the prediction target (
y
)
next
8.1.
Toy datasets
This Page
Show Source
© Copyright 2007 - 2025, scikit-learn developers (BSD License).